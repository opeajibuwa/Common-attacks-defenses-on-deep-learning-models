{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39m## Forward Pass\u001b[39;00m\n\u001b[0;32m    142\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 143\u001b[0m scores \u001b[39m=\u001b[39m model_resnet(data)\n\u001b[0;32m    144\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(scores, targets)\n\u001b[0;32m    145\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Opeyemi\\Desktop\\deep_learning\\codes\\codes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 77\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     78\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(out)\n\u001b[0;32m     79\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n",
      "File \u001b[1;32mc:\\Users\\Opeyemi\\Desktop\\deep_learning\\codes\\codes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Opeyemi\\Desktop\\deep_learning\\codes\\codes\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Opeyemi\\Desktop\\deep_learning\\codes\\codes\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import torchplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# define the model\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        DROPOUT = 0.1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes),\n",
    "                nn.Dropout(DROPOUT)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.dropout(self.bn1(self.conv1(x))))\n",
    "        out = self.dropout(self.bn2(self.conv2(out)))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "norm = transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        norm,\n",
    "    ])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])\n",
    "\n",
    "# download the CIFAR_10 dataset from torchvision API to the local directory\n",
    "train_val_dataset = datasets.CIFAR10(root=\"./datasets/\", train=True, download=False, transform=train_transforms)\n",
    "test_dataset = datasets.CIFAR10(root=\"./datasets/\", train=False, download=False, transform=test_transforms)\n",
    "\n",
    "# split the dataset into training, validation and testing sets\n",
    "train_size = int(0.9 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
    "\n",
    "# create the dataloader for training, validation and testing sets\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# initialize the model, loss function, optimizer and evaluation metrics\n",
    "model_resnet = ResNet18()\n",
    "# optimizer = torch.optim.SGD(model_resnet.parameters(), momentum=0.9, lr=0.01)\n",
    "optimizer = torch.optim.Adam(model_resnet.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "# train the model\n",
    "epoch = 90\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # device-agnostic setup\n",
    "accuracy = accuracy.to(device)\n",
    "model_resnet = model_resnet.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_list = []\n",
    "\n",
    "for epochs in range(epoch): #I decided to train the model for 50 epochs\n",
    "    loss_ep = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_dataloader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        ## Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        scores = model_resnet(data)\n",
    "        loss = loss_fn(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ep += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epochs+1} ] Loss : {loss_ep/len(train_dataloader):.6f}\")\n",
    "    \n",
    "    if (epochs + 1) in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "        train_losses.append(loss_ep / len(train_dataloader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss_ep = 0\n",
    "            for batch_idx, (data, targets) in enumerate(val_dataloader):\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "                scores = model_resnet(data)\n",
    "                val_loss = loss_fn(scores, targets)\n",
    "                val_loss_ep += val_loss.item()\n",
    "            val_losses.append(val_loss_ep / len(val_dataloader))\n",
    "        \n",
    "        epochs_list.append(epochs + 1)\n",
    "        \n",
    "        # Save the model at the desired epoch\n",
    "        torch.save(model_resnet.state_dict(), f\"model_epoch_{epochs+1}.pth\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for batch_idx, (data, targets) in enumerate(val_dataloader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            ## Forward Pass\n",
    "            scores = model_resnet(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(f\"Epoch {epochs+1} ] Accuracy : {float(num_correct) / float(num_samples) * 100:.2f}%\")\n",
    "\n",
    "dest = Path(\"figures\")\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "dest1 = dest / \"resnet_cifar10_wt_diff_batches.png\"\n",
    "plt.plot(epochs_list, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_list, val_losses, label='Val Loss')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig(dest1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 1.2\n",
    "EPSILON = 10\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 30\n",
    "LR = 1e-3 # replace with 0.2\n",
    "BATCH_SIZE = 32\n",
    "MAX_PHYSICAL_BATCH_SIZE = 32\n",
    "\n",
    "norm = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "train_transforms = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.ToTensor(), norm,])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), norm,])\n",
    "\n",
    "# download the CIFAR_10 dataset from torchvision API to the local directory\n",
    "train_val_dataset = datasets.CIFAR10(root=\"./datasets/\", train=True, download=False, transform=train_transforms)\n",
    "test_dataset = datasets.CIFAR10(root=\"./datasets/\", train=False, download=False, transform=test_transforms)\n",
    "\n",
    "# create the dataloader for training, validation and testing sets\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# initialize the model\n",
    "model = models.resnet18(num_classes=10)\n",
    "model = model.to(device)\n",
    "\n",
    "# check and fix the layer incompatibility issues \n",
    "errors = ModuleValidator.validate(model, strict=False)\n",
    "(errors[-5:])\n",
    "model = ModuleValidator.fix(model)\n",
    "ModuleValidator.validate(model, strict=False)\n",
    "\n",
    "# set the optimizer and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LR)\n",
    "\n",
    "# define a util function to calculate the accuracy\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# attach the privacy engine initialized with the privacy hyperparameters defined earlier\n",
    "privacy_engine = PrivacyEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sigma=0.453338623046875 and C=1.2\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "    \n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_loader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "\n",
    "        for i, (images, target) in enumerate(memory_safe_data_loader):   \n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 200 == 0:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "                print(\n",
    "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
    "                )\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "    top1_avg = np.mean(top1_acc)\n",
    "\n",
    "    print(\n",
    "        f\"\\tTest set:\"\n",
    "        f\"Loss: {np.mean(losses):.6f} \"\n",
    "        f\"Acc: {top1_avg * 100:.6f} \"\n",
    "    )\n",
    "    return np.mean(top1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3185cec487492788a0b17e80d0c5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/30 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 1 \tLoss: 2.300409 Acc@1: 14.429962 (ε = 2.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.271677 Acc@1: 15.576773 (ε = 3.27, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.249775 Acc@1: 16.260741 (ε = 3.50, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.235318 Acc@1: 16.738941 (ε = 3.66, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.222967 Acc@1: 17.340060 (ε = 3.79, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.213661 Acc@1: 17.764591 (ε = 3.90, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.202575 Acc@1: 18.355174 (ε = 3.99, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.194030 Acc@1: 18.848808 (ε = 4.07, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.182669 Acc@1: 19.400490 (ε = 4.14, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 2.174199 Acc@1: 19.800762 (ε = 4.21, δ = 1e-05)\n",
      "\tTest set:Loss: 2.040468 Acc: 25.249601 \n",
      "\tTrain Epoch: 2 \tLoss: 2.083283 Acc@1: 23.046103 (ε = 4.29, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.093376 Acc@1: 22.834250 (ε = 4.35, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.100100 Acc@1: 22.494610 (ε = 4.40, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.094087 Acc@1: 22.910970 (ε = 4.46, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.086156 Acc@1: 23.206786 (ε = 4.51, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.076748 Acc@1: 23.805424 (ε = 4.56, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.071501 Acc@1: 23.913182 (ε = 4.61, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.069461 Acc@1: 23.989886 (ε = 4.65, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.066951 Acc@1: 24.195218 (ε = 4.70, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 2.066556 Acc@1: 24.239597 (ε = 4.74, δ = 1e-05)\n",
      "\tTest set:Loss: 2.028753 Acc: 26.537540 \n",
      "\tTrain Epoch: 3 \tLoss: 2.032118 Acc@1: 26.626811 (ε = 4.79, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.034409 Acc@1: 25.690858 (ε = 4.83, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.029494 Acc@1: 26.099985 (ε = 4.87, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.031096 Acc@1: 26.040976 (ε = 4.90, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.035974 Acc@1: 25.932838 (ε = 4.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.037599 Acc@1: 25.792531 (ε = 4.98, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.036351 Acc@1: 25.768801 (ε = 5.01, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.034830 Acc@1: 25.661639 (ε = 5.05, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.032562 Acc@1: 25.866031 (ε = 5.09, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 2.030474 Acc@1: 25.990289 (ε = 5.12, δ = 1e-05)\n",
      "\tTest set:Loss: 1.998103 Acc: 28.604233 \n",
      "\tTrain Epoch: 4 \tLoss: 2.008724 Acc@1: 26.828939 (ε = 5.16, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.009600 Acc@1: 26.715121 (ε = 5.19, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.006175 Acc@1: 27.188890 (ε = 5.22, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 1.998283 Acc@1: 27.490852 (ε = 5.26, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.004682 Acc@1: 27.553747 (ε = 5.29, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.006640 Acc@1: 27.422738 (ε = 5.32, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.009827 Acc@1: 27.209742 (ε = 5.35, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.010579 Acc@1: 27.143955 (ε = 5.38, δ = 1e-05)\n",
      "\tTrain Epoch: 4 \tLoss: 2.009084 Acc@1: 27.292439 (ε = 5.41, δ = 1e-05)\n",
      "\tTest set:Loss: 1.964728 Acc: 30.630990 \n",
      "\tTrain Epoch: 5 \tLoss: 1.994415 Acc@1: 29.326052 (ε = 5.47, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.993056 Acc@1: 28.895723 (ε = 5.50, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.999112 Acc@1: 28.502407 (ε = 5.53, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.995946 Acc@1: 28.481693 (ε = 5.55, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.995667 Acc@1: 28.519412 (ε = 5.58, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.995447 Acc@1: 28.461761 (ε = 5.61, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.997844 Acc@1: 28.406000 (ε = 5.64, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 2.000490 Acc@1: 28.428327 (ε = 5.66, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.998284 Acc@1: 28.495935 (ε = 5.69, δ = 1e-05)\n",
      "\tTrain Epoch: 5 \tLoss: 1.994543 Acc@1: 28.664930 (ε = 5.72, δ = 1e-05)\n",
      "\tTest set:Loss: 1.964377 Acc: 31.010383 \n",
      "\tTrain Epoch: 6 \tLoss: 1.991135 Acc@1: 29.360089 (ε = 5.75, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.973487 Acc@1: 29.825462 (ε = 5.77, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.971567 Acc@1: 29.814854 (ε = 5.80, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.970231 Acc@1: 29.929291 (ε = 5.82, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.977228 Acc@1: 29.621886 (ε = 5.85, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.980399 Acc@1: 29.474235 (ε = 5.87, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.977051 Acc@1: 29.561121 (ε = 5.90, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.974917 Acc@1: 29.569419 (ε = 5.92, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.972428 Acc@1: 29.739870 (ε = 5.95, δ = 1e-05)\n",
      "\tTrain Epoch: 6 \tLoss: 1.971217 Acc@1: 29.786045 (ε = 5.97, δ = 1e-05)\n",
      "\tTest set:Loss: 1.925219 Acc: 31.898962 \n",
      "\tTrain Epoch: 7 \tLoss: 1.942139 Acc@1: 31.153654 (ε = 6.00, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.938238 Acc@1: 31.267921 (ε = 6.02, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.938887 Acc@1: 31.387260 (ε = 6.05, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.942016 Acc@1: 31.201754 (ε = 6.07, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.940497 Acc@1: 31.221765 (ε = 6.09, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.945553 Acc@1: 31.252179 (ε = 6.12, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.943774 Acc@1: 31.420141 (ε = 6.14, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.947958 Acc@1: 31.391676 (ε = 6.16, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.947380 Acc@1: 31.371594 (ε = 6.19, δ = 1e-05)\n",
      "\tTrain Epoch: 7 \tLoss: 1.948205 Acc@1: 31.430528 (ε = 6.21, δ = 1e-05)\n",
      "\tTest set:Loss: 1.965738 Acc: 33.246805 \n",
      "\tTrain Epoch: 8 \tLoss: 1.959017 Acc@1: 31.031862 (ε = 6.24, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.957253 Acc@1: 31.651345 (ε = 6.27, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.957051 Acc@1: 31.534820 (ε = 6.29, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.954420 Acc@1: 31.540179 (ε = 6.31, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.958136 Acc@1: 31.538570 (ε = 6.33, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.956208 Acc@1: 31.792276 (ε = 6.36, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.952292 Acc@1: 31.779112 (ε = 6.38, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.956367 Acc@1: 31.672664 (ε = 6.40, δ = 1e-05)\n",
      "\tTrain Epoch: 8 \tLoss: 1.955967 Acc@1: 31.650438 (ε = 6.42, δ = 1e-05)\n",
      "\tTest set:Loss: 1.925310 Acc: 34.215256 \n",
      "\tTrain Epoch: 9 \tLoss: 1.925498 Acc@1: 31.844696 (ε = 6.46, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.921853 Acc@1: 32.355495 (ε = 6.48, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.925953 Acc@1: 32.264020 (ε = 6.50, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.933231 Acc@1: 32.016755 (ε = 6.53, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.931909 Acc@1: 32.116133 (ε = 6.55, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.929753 Acc@1: 32.358299 (ε = 6.57, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.931310 Acc@1: 32.352784 (ε = 6.59, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.928242 Acc@1: 32.445839 (ε = 6.61, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.929355 Acc@1: 32.424751 (ε = 6.63, δ = 1e-05)\n",
      "\tTrain Epoch: 9 \tLoss: 1.931421 Acc@1: 32.459286 (ε = 6.65, δ = 1e-05)\n",
      "\tTest set:Loss: 1.873460 Acc: 35.672923 \n",
      "\tTrain Epoch: 10 \tLoss: 1.927839 Acc@1: 33.475122 (ε = 6.68, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.935836 Acc@1: 33.354772 (ε = 6.70, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.937606 Acc@1: 32.943526 (ε = 6.72, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.939620 Acc@1: 32.858229 (ε = 6.74, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.942795 Acc@1: 32.832975 (ε = 6.76, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.943540 Acc@1: 32.810457 (ε = 6.78, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.946045 Acc@1: 32.817569 (ε = 6.80, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.948842 Acc@1: 32.759893 (ε = 6.82, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.947696 Acc@1: 32.856196 (ε = 6.84, δ = 1e-05)\n",
      "\tTrain Epoch: 10 \tLoss: 1.946695 Acc@1: 32.850367 (ε = 6.86, δ = 1e-05)\n",
      "\tTest set:Loss: 1.967001 Acc: 34.005591 \n",
      "\tTrain Epoch: 11 \tLoss: 1.978515 Acc@1: 31.431172 (ε = 6.88, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.975641 Acc@1: 31.872891 (ε = 6.90, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.967831 Acc@1: 32.235718 (ε = 6.92, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.966889 Acc@1: 32.302876 (ε = 6.94, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.969852 Acc@1: 32.499894 (ε = 6.96, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.968903 Acc@1: 32.385070 (ε = 6.98, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.966910 Acc@1: 32.312311 (ε = 7.00, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.968158 Acc@1: 32.301409 (ε = 7.02, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.970038 Acc@1: 32.265112 (ε = 7.04, δ = 1e-05)\n",
      "\tTrain Epoch: 11 \tLoss: 1.971387 Acc@1: 32.286575 (ε = 7.05, δ = 1e-05)\n",
      "\tTest set:Loss: 1.932411 Acc: 35.203674 \n",
      "\tTrain Epoch: 12 \tLoss: 1.963495 Acc@1: 32.858700 (ε = 7.07, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.941821 Acc@1: 33.562702 (ε = 7.09, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.961453 Acc@1: 32.939828 (ε = 7.11, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.958843 Acc@1: 33.113531 (ε = 7.13, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.955973 Acc@1: 33.231324 (ε = 7.15, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.956841 Acc@1: 33.274384 (ε = 7.17, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.953110 Acc@1: 33.418936 (ε = 7.19, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.953408 Acc@1: 33.336528 (ε = 7.20, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.953500 Acc@1: 33.204283 (ε = 7.22, δ = 1e-05)\n",
      "\tTrain Epoch: 12 \tLoss: 1.951580 Acc@1: 33.159654 (ε = 7.24, δ = 1e-05)\n",
      "\tTest set:Loss: 1.964557 Acc: 34.315096 \n",
      "\tTrain Epoch: 13 \tLoss: 1.961571 Acc@1: 33.336522 (ε = 7.26, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.943258 Acc@1: 33.021908 (ε = 7.28, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.957633 Acc@1: 32.693209 (ε = 7.30, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.949638 Acc@1: 32.883384 (ε = 7.32, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.947955 Acc@1: 32.764661 (ε = 7.34, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.943741 Acc@1: 32.857395 (ε = 7.35, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.945093 Acc@1: 32.885289 (ε = 7.37, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.944089 Acc@1: 33.099216 (ε = 7.39, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.947372 Acc@1: 33.157005 (ε = 7.41, δ = 1e-05)\n",
      "\tTrain Epoch: 13 \tLoss: 1.950109 Acc@1: 33.147543 (ε = 7.42, δ = 1e-05)\n",
      "\tTest set:Loss: 1.991999 Acc: 35.003994 \n",
      "\tTrain Epoch: 14 \tLoss: 1.950476 Acc@1: 33.952896 (ε = 7.45, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.974841 Acc@1: 34.140183 (ε = 7.46, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.981280 Acc@1: 33.629046 (ε = 7.48, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.984199 Acc@1: 33.030256 (ε = 7.50, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.984241 Acc@1: 32.837492 (ε = 7.52, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.984572 Acc@1: 32.824703 (ε = 7.53, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.983119 Acc@1: 32.800060 (ε = 7.55, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.982491 Acc@1: 32.744081 (ε = 7.57, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.980105 Acc@1: 32.790105 (ε = 7.59, δ = 1e-05)\n",
      "\tTrain Epoch: 14 \tLoss: 1.979261 Acc@1: 32.861489 (ε = 7.60, δ = 1e-05)\n",
      "\tTest set:Loss: 1.999916 Acc: 34.854233 \n",
      "\tTrain Epoch: 15 \tLoss: 1.947817 Acc@1: 33.715761 (ε = 7.62, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.938516 Acc@1: 33.834033 (ε = 7.64, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.936147 Acc@1: 33.569260 (ε = 7.66, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.943923 Acc@1: 33.438100 (ε = 7.67, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.947872 Acc@1: 33.288438 (ε = 7.69, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.947404 Acc@1: 33.346256 (ε = 7.71, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.951916 Acc@1: 33.390941 (ε = 7.72, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.954454 Acc@1: 33.493448 (ε = 7.74, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.947026 Acc@1: 33.610008 (ε = 7.76, δ = 1e-05)\n",
      "\tTrain Epoch: 15 \tLoss: 1.949962 Acc@1: 33.647555 (ε = 7.77, δ = 1e-05)\n",
      "\tTest set:Loss: 2.013554 Acc: 34.634585 \n",
      "\tTrain Epoch: 16 \tLoss: 1.978448 Acc@1: 32.759446 (ε = 7.80, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.956678 Acc@1: 33.379291 (ε = 7.81, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.939717 Acc@1: 33.913392 (ε = 7.83, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.942150 Acc@1: 34.040296 (ε = 7.85, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.937521 Acc@1: 34.207205 (ε = 7.86, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.934470 Acc@1: 34.237073 (ε = 7.88, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.934328 Acc@1: 34.251889 (ε = 7.90, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.933231 Acc@1: 34.388871 (ε = 7.91, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.936752 Acc@1: 34.208342 (ε = 7.93, δ = 1e-05)\n",
      "\tTrain Epoch: 16 \tLoss: 1.939563 Acc@1: 34.117619 (ε = 7.94, δ = 1e-05)\n",
      "\tTest set:Loss: 1.956415 Acc: 36.741214 \n",
      "\tTrain Epoch: 17 \tLoss: 1.948596 Acc@1: 34.556863 (ε = 7.96, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.942609 Acc@1: 34.199288 (ε = 7.98, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.938335 Acc@1: 34.373557 (ε = 8.00, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.941831 Acc@1: 34.036015 (ε = 8.01, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.940873 Acc@1: 33.901863 (ε = 8.03, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.938458 Acc@1: 34.055991 (ε = 8.04, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.939486 Acc@1: 34.070609 (ε = 8.06, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.943256 Acc@1: 33.944325 (ε = 8.08, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.941075 Acc@1: 33.991747 (ε = 8.09, δ = 1e-05)\n",
      "\tTrain Epoch: 17 \tLoss: 1.939885 Acc@1: 34.024438 (ε = 8.11, δ = 1e-05)\n",
      "\tTest set:Loss: 1.914103 Acc: 35.782748 \n",
      "\tTrain Epoch: 18 \tLoss: 1.951498 Acc@1: 33.722277 (ε = 8.13, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.940273 Acc@1: 33.682820 (ε = 8.14, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.930557 Acc@1: 33.952676 (ε = 8.16, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.927618 Acc@1: 34.154884 (ε = 8.17, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.932072 Acc@1: 33.987796 (ε = 8.19, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.927132 Acc@1: 34.333897 (ε = 8.20, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.931368 Acc@1: 34.133749 (ε = 8.22, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.930929 Acc@1: 34.218188 (ε = 8.24, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.930537 Acc@1: 34.378952 (ε = 8.25, δ = 1e-05)\n",
      "\tTrain Epoch: 18 \tLoss: 1.929358 Acc@1: 34.462989 (ε = 8.27, δ = 1e-05)\n",
      "\tTest set:Loss: 2.035263 Acc: 36.531550 \n",
      "\tTrain Epoch: 19 \tLoss: 1.966915 Acc@1: 34.116264 (ε = 8.29, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.947418 Acc@1: 34.739518 (ε = 8.30, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.944004 Acc@1: 34.763609 (ε = 8.32, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.940394 Acc@1: 34.823826 (ε = 8.33, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.942613 Acc@1: 34.692426 (ε = 8.35, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.939563 Acc@1: 34.796077 (ε = 8.36, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.936890 Acc@1: 34.847763 (ε = 8.38, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.936962 Acc@1: 34.970429 (ε = 8.39, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.933849 Acc@1: 35.121008 (ε = 8.41, δ = 1e-05)\n",
      "\tTrain Epoch: 19 \tLoss: 1.934579 Acc@1: 35.071164 (ε = 8.42, δ = 1e-05)\n",
      "\tTest set:Loss: 2.000264 Acc: 36.142173 \n",
      "\tTrain Epoch: 20 \tLoss: 1.959361 Acc@1: 33.912598 (ε = 8.44, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.934763 Acc@1: 34.816766 (ε = 8.46, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.923666 Acc@1: 35.067185 (ε = 8.47, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.931421 Acc@1: 34.964407 (ε = 8.49, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.936983 Acc@1: 35.008395 (ε = 8.50, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.934944 Acc@1: 35.153746 (ε = 8.52, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.942751 Acc@1: 35.128261 (ε = 8.53, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.945812 Acc@1: 35.047290 (ε = 8.55, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.948232 Acc@1: 34.868541 (ε = 8.56, δ = 1e-05)\n",
      "\tTrain Epoch: 20 \tLoss: 1.944953 Acc@1: 34.920649 (ε = 8.58, δ = 1e-05)\n",
      "\tTest set:Loss: 2.093692 Acc: 35.662939 \n",
      "\tTrain Epoch: 21 \tLoss: 1.926635 Acc@1: 34.916601 (ε = 8.60, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.938746 Acc@1: 34.976791 (ε = 8.61, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.923065 Acc@1: 35.225995 (ε = 8.63, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.907236 Acc@1: 35.516911 (ε = 8.64, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.906330 Acc@1: 35.572848 (ε = 8.65, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.908847 Acc@1: 35.621823 (ε = 8.67, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.910601 Acc@1: 35.666514 (ε = 8.68, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.905954 Acc@1: 35.771242 (ε = 8.70, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.905946 Acc@1: 35.738293 (ε = 8.71, δ = 1e-05)\n",
      "\tTrain Epoch: 21 \tLoss: 1.906981 Acc@1: 35.681450 (ε = 8.73, δ = 1e-05)\n",
      "\tTest set:Loss: 1.908054 Acc: 37.789537 \n",
      "\tTrain Epoch: 22 \tLoss: 1.877153 Acc@1: 36.428100 (ε = 8.75, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.899686 Acc@1: 35.518885 (ε = 8.76, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.896391 Acc@1: 35.685692 (ε = 8.78, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.894160 Acc@1: 35.995341 (ε = 8.79, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.891640 Acc@1: 35.814866 (ε = 8.81, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.889058 Acc@1: 36.041954 (ε = 8.82, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.885138 Acc@1: 36.177860 (ε = 8.84, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.887055 Acc@1: 36.294320 (ε = 8.85, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.889321 Acc@1: 36.262452 (ε = 8.86, δ = 1e-05)\n",
      "\tTrain Epoch: 22 \tLoss: 1.890029 Acc@1: 36.108746 (ε = 8.88, δ = 1e-05)\n",
      "\tTest set:Loss: 1.936117 Acc: 36.781150 \n",
      "\tTrain Epoch: 23 \tLoss: 1.929182 Acc@1: 35.918610 (ε = 8.90, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.916839 Acc@1: 36.107905 (ε = 8.91, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.927535 Acc@1: 36.184551 (ε = 8.93, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.919356 Acc@1: 36.209665 (ε = 8.94, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.916609 Acc@1: 36.138187 (ε = 8.95, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.923099 Acc@1: 35.918489 (ε = 8.97, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.922940 Acc@1: 35.869359 (ε = 8.98, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.921793 Acc@1: 35.950014 (ε = 9.00, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.921596 Acc@1: 35.878296 (ε = 9.01, δ = 1e-05)\n",
      "\tTrain Epoch: 23 \tLoss: 1.924273 Acc@1: 35.841324 (ε = 9.03, δ = 1e-05)\n",
      "\tTest set:Loss: 1.936215 Acc: 37.210463 \n",
      "\tTrain Epoch: 24 \tLoss: 1.918862 Acc@1: 36.197060 (ε = 9.04, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.917575 Acc@1: 35.884717 (ε = 9.06, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.914312 Acc@1: 35.685571 (ε = 9.07, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.913832 Acc@1: 35.997777 (ε = 9.08, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.912892 Acc@1: 35.863480 (ε = 9.10, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.913022 Acc@1: 35.817632 (ε = 9.11, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.916288 Acc@1: 35.693938 (ε = 9.13, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.912898 Acc@1: 35.735551 (ε = 9.14, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.911192 Acc@1: 35.760409 (ε = 9.15, δ = 1e-05)\n",
      "\tTrain Epoch: 24 \tLoss: 1.907825 Acc@1: 35.914870 (ε = 9.17, δ = 1e-05)\n",
      "\tTest set:Loss: 1.929075 Acc: 38.328674 \n",
      "\tTrain Epoch: 25 \tLoss: 1.881305 Acc@1: 37.258729 (ε = 9.19, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.889266 Acc@1: 37.832335 (ε = 9.20, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.892764 Acc@1: 37.401766 (ε = 9.21, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.888370 Acc@1: 37.264303 (ε = 9.23, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.902009 Acc@1: 36.987733 (ε = 9.24, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.904886 Acc@1: 36.740671 (ε = 9.25, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.903997 Acc@1: 36.710356 (ε = 9.27, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.903658 Acc@1: 36.649319 (ε = 9.28, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.906424 Acc@1: 36.573481 (ε = 9.30, δ = 1e-05)\n",
      "\tTrain Epoch: 25 \tLoss: 1.908444 Acc@1: 36.544607 (ε = 9.31, δ = 1e-05)\n",
      "\tTest set:Loss: 1.902133 Acc: 38.558307 \n",
      "\tTrain Epoch: 26 \tLoss: 1.923973 Acc@1: 36.190455 (ε = 9.33, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.930999 Acc@1: 36.150695 (ε = 9.34, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.928186 Acc@1: 36.238507 (ε = 9.35, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.928583 Acc@1: 36.397330 (ε = 9.37, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.926776 Acc@1: 36.302345 (ε = 9.38, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.927359 Acc@1: 36.326760 (ε = 9.40, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.925710 Acc@1: 36.394229 (ε = 9.41, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.923224 Acc@1: 36.325609 (ε = 9.42, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.923714 Acc@1: 36.335411 (ε = 9.44, δ = 1e-05)\n",
      "\tTrain Epoch: 26 \tLoss: 1.927399 Acc@1: 36.257670 (ε = 9.45, δ = 1e-05)\n",
      "\tTest set:Loss: 1.869457 Acc: 38.797923 \n",
      "\tTrain Epoch: 27 \tLoss: 1.923138 Acc@1: 36.522305 (ε = 9.47, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.903949 Acc@1: 36.679653 (ε = 9.48, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.904478 Acc@1: 36.476654 (ε = 9.49, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.908957 Acc@1: 36.258627 (ε = 9.51, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.910345 Acc@1: 36.182916 (ε = 9.52, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.907512 Acc@1: 36.235171 (ε = 9.53, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.907223 Acc@1: 36.331404 (ε = 9.55, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.911385 Acc@1: 36.180587 (ε = 9.56, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.915432 Acc@1: 36.060211 (ε = 9.58, δ = 1e-05)\n",
      "\tTrain Epoch: 27 \tLoss: 1.914077 Acc@1: 36.027362 (ε = 9.59, δ = 1e-05)\n",
      "\tTest set:Loss: 1.884186 Acc: 37.669728 \n",
      "\tTrain Epoch: 28 \tLoss: 1.896830 Acc@1: 35.613589 (ε = 9.60, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.888476 Acc@1: 36.294613 (ε = 9.62, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.893091 Acc@1: 36.381167 (ε = 9.63, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.900207 Acc@1: 36.615245 (ε = 9.64, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.906816 Acc@1: 36.474226 (ε = 9.66, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.907529 Acc@1: 36.445179 (ε = 9.67, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.906629 Acc@1: 36.467411 (ε = 9.68, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.910082 Acc@1: 36.504748 (ε = 9.70, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.911825 Acc@1: 36.523424 (ε = 9.71, δ = 1e-05)\n",
      "\tTrain Epoch: 28 \tLoss: 1.911766 Acc@1: 36.587512 (ε = 9.72, δ = 1e-05)\n",
      "\tTest set:Loss: 2.054450 Acc: 37.350240 \n",
      "\tTrain Epoch: 29 \tLoss: 1.921724 Acc@1: 37.196550 (ε = 9.74, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.924126 Acc@1: 37.306409 (ε = 9.75, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.937404 Acc@1: 36.990621 (ε = 9.76, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.931472 Acc@1: 37.141088 (ε = 9.78, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.930990 Acc@1: 37.161157 (ε = 9.79, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.935215 Acc@1: 36.994726 (ε = 9.80, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.926390 Acc@1: 37.134214 (ε = 9.82, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.926244 Acc@1: 37.290780 (ε = 9.83, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.929484 Acc@1: 37.250308 (ε = 9.84, δ = 1e-05)\n",
      "\tTrain Epoch: 29 \tLoss: 1.929436 Acc@1: 37.153975 (ε = 9.86, δ = 1e-05)\n",
      "\tTest set:Loss: 1.932681 Acc: 39.227236 \n",
      "\tTrain Epoch: 30 \tLoss: 1.948106 Acc@1: 36.865535 (ε = 9.87, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.934629 Acc@1: 36.781201 (ε = 9.89, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.941331 Acc@1: 36.433687 (ε = 9.90, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.944907 Acc@1: 36.421512 (ε = 9.91, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.939046 Acc@1: 36.576232 (ε = 9.92, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.940838 Acc@1: 36.668079 (ε = 9.94, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.945334 Acc@1: 36.609288 (ε = 9.95, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.938948 Acc@1: 36.659933 (ε = 9.96, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.933019 Acc@1: 36.781899 (ε = 9.98, δ = 1e-05)\n",
      "\tTrain Epoch: 30 \tLoss: 1.929553 Acc@1: 36.814096 (ε = 9.99, δ = 1e-05)\n",
      "\tTest set:Loss: 1.988954 Acc: 38.338658 \n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    model = train(model, train_loader, optimizer, epoch + 1, device)\n",
    "\n",
    "    # test the network on test data and save the best model\n",
    "    top1_acc = test(model, test_dataloader, device)\n",
    "\n",
    "    if top1_acc > best_acc:\n",
    "        best_acc = top1_acc\n",
    "        torch.save(model.state_dict(), f\"resnet_cifar10_eps_{EPSILON}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3922723642172524"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps = [1, 2, 4, 8, 10]\n",
    "acc = [0.09581, 0.10157, 0.15450, 0.17678, 0.24237]\n",
    "plt.plot(eps, acc, label=\"epsilon vs. model's accuracy\")\n",
    "plt.xlabel('epilson')\n",
    "plt.ylabel(\"model's accuracy\")\n",
    "plt.title(\"epilson vs model's accuracy for VGG16-FMNIST model\")\n",
    "plt.legend()\n",
    "plt.savefig(\"eps_acc_vgg16_fmnist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: saved_dp_models\\resnet_cifar10_eps_1.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = models.resnet18(num_classes=10).to(device)  # Replace DEVICE with your target device\n",
    "    saved_state_dict = torch.load(model_path)\n",
    "\n",
    "    # Check for missing and unexpected keys\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    for key in saved_state_dict.keys():\n",
    "        if key not in model.state_dict():\n",
    "            unexpected_keys.append(key)\n",
    "    for key in model.state_dict().keys():\n",
    "        if key not in saved_state_dict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    # Fix the missing and unexpected keys\n",
    "    for missing_key in missing_keys:\n",
    "        saved_state_dict[missing_key] = model.state_dict()[missing_key]\n",
    "    for unexpected_key in unexpected_keys:\n",
    "        del saved_state_dict[unexpected_key]\n",
    "\n",
    "    # Load the modified state dictionary\n",
    "    model.load_state_dict(saved_state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "model_path = Path(\"saved_dp_models/resnet_cifar10_eps_1.pth\")  # Replace with the actual path to your saved model file\n",
    "resnet_model = load_model(model_path)\n",
    "print(\"Loaded model:\", model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
